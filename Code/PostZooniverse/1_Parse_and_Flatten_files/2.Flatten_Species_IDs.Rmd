---
title: "Flattening our Zooniverse files"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

**THIS FILE IS FOR PARSING THE JSON DATA FROM THE "SPECIES IDENTIFICATION" WORKFLOWS. USE THE FILE "FLATTEN_IS_THERE_ANIMAL_ZOONIVERSE.RMD" TO PARSE ANIMAL PRESENCE/ABSENCE DATA.**

## Overview
Output from classifications on the [Zooniverse](https://zooniverse.com) come encoded in *JSON format*.  While most of the columns are straightforward to interpret, two critical columns are not.  The first is the **annotation column**.  Within it are the species identification and the answers to all of the questions for each event. Likewise, the **subject_data** column contains all of the information from the manifest, such as the forest type, photo batch, camera number, and forest name.  It would be good to be able to pull all of this information. We especially need it if we plan to remove e.g. images taken on malfunctioning cameras or otherwise suspect images.
 
Processing the data requires five steps:
1. Selecting the correct classification file.  That file is obtained from the `1.Parse_workflows.Rmd` file in the previous step. **For this code to run, we are assuming you have a single classification file containing only one workflow and single workflow version.**

2. Parsing the annotations column. The annotations column is the column in the zooniverse classification file that includes all of the answers to the questions in the zooniverse workflow, including what the species is and what the animals are doing.

3. Parsing the subject_data column. The subject_data column is the column from the zooniverse classification file that includes information about each subject set from the manifest file.

4. Join the annotation & subject_data together into a data frame for further data analysis.

5. Write the output to a folder called "Flattened" in the PostZooniverse Output folder.


### Turn on packages and bring in the data and r scripts
We need to load desired packages and also source custom functions  so that they will run. The tidyjson package may have to be loaded from this [github](https://github.com/sailthru/tidyjson) site. Look [here](https://cran.r-project.org/web/packages/githubinstall/vignettes/githubinstall.html) for some hints as to how to do that.

```{r, message=FALSE, echo=FALSE}
rm(list = ls())
library(here)
library(tidyverse)
library(tidyjson)#may get error; if so, load from github
library(jsonlite)
library(magrittr) #allows piping beyond tidyverse
library(dplyr)
library(stringr)
library(tidyr)
library(lubridate)
library(rstudioapi)


myFunctions<-list.files(here("Code", "Functions"), pattern = "R") 
#points to files that need to be sourced

sapply(here("Code", "Functions", myFunctions), source) #sources all of the functions in the Functions folder.
```

###Step 1. Choose the classification file to work with.
We have multiple classification files as a result of the `1.Parse_workflows.Rmd` file operation. 

Here we point to the folder containing those files and obtain information about the file. 

```{r}
DF<-choose_classification_file()
data<-DF[[1]]
filename<-DF[[2]]
```

### Step 2. Obtain metadata about the file

Let's gather some information about this data file and create a data frame in which to gather it.

```{r}
File_metadata <- data.frame(Parameter = character(),
                            Value = character()) #intialize empty dataframe

File_metadata[1,] <- c("File name", filename)
File_metadata[2,] <- c("Workflow ID", unique(data$workflow_id))
File_metadata[3,] <-c("Workflow name", unique(data$workflow_name))
File_metadata[4,] <-c("Workflow version", unique(data$workflow_version))
File_metadata[5,] <-c("Num unflattened rows", length(data$classification_id))

```



### Step 3. Parse the annotations column
We now flatten the Annotations column by calling the  `flatten_annotations` function. 

```{r}
flat_file<-flatten_annotations(data) #this is slow; could we write faster code?  A problem for another day.

File_metadata[6,] <- c("Num unique classifications", length(unique(data$classification_id)))
File_metadata[7,] <- c("Num unique subjects", length(unique(data$subject_ids)))
```

### Step 4. Parse the subjects column

Now we can parse the data.


```{r}
subjects<-flatten_subjects(data)
```

I've updated flatten_subjects so that it reports a lot of columns, parsing all of the subject columns regardless of the format of the manifest that was uploaded to the zooniverse. So now we can sort by project and isolate data from different manifests that use the same workflow. 


Notice that if flat_file may have fewer rows than subjects it is because for some subset of the data (equal in number to the difference in length b/w the subjects data and the flat_file data) no identification was made, so the annotations column has no information.

###Step 4. Merge the annotations and subjects data
Now that we have flattened both the annotations and subject_data columns, we would like to generate one large data frame with all of the data to export and analyze (using different R scripts!).  To do so, we need to join the two data frames.  Joining will **only work** if you have a column, named identically in both data frames, on which the join will work.

The join itself is pretty easy:
```{r}
Flattened<-left_join(flat_file, subjects, by='classification_id')

# Now move subject_ids column to left most position

Flattened<-Flattened %>% relocate(subject_ids)
```

Now Save our result! This will go in the flattened folder within the PostZooniverse outputs.
```{r}
output<-here("Output", "PostZooniverse", "Flattened", "Species_ID")

myFile<-str_remove(filename, ".csv") #drop the .csv temporarily

write.csv(Flattened,  paste0(output,"/",myFile,"_flat_", Sys.Date(), ".csv"))

#and write the metadata
write.csv(File_metadata, paste0(output,"/",myFile,"_fMeta_", Sys.Date(), ".csv"))

```



